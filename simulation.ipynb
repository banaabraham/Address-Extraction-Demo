{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "In this competition, you’ll work on addresses collected by us to build a model to correctly extract Point of Interest (POI) Names and Street Names from unformatted Indonesia addresses.\n",
    "\n",
    "Participants are expected to build their own model for this competition, submissions by teams which directly call any third party APIs on the test set will not be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_address</th>\n",
       "      <th>POI/street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jl kapuk timur delta sili iii lippo cika 11 a ...</td>\n",
       "      <td>/jl kapuk timur delta sili iii lippo cika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aye, jati sampurna</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>setu siung 119 rt 5 1 13880 cipayung</td>\n",
       "      <td>/siung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>toko dita, kertosono</td>\n",
       "      <td>toko dita/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>jl. orde baru</td>\n",
       "      <td>/jl. orde baru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>pawon solo, aryo beba,</td>\n",
       "      <td>pawon solo/aryo beba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>mawar iii, no 19 telukjambe timur</td>\n",
       "      <td>/mawar iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>soek - hatta 60 kepuhkembeng peterongan</td>\n",
       "      <td>/soek - hatta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>yaya pelayanan halieluyah, tebet raya, 30d rw ...</td>\n",
       "      <td>yayasan pelayanan halieluyah/tebet raya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>purman tanj beri no al 0 4 pasar rebo</td>\n",
       "      <td>/beri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        raw_address  \\\n",
       "0      0  jl kapuk timur delta sili iii lippo cika 11 a ...   \n",
       "1      1                                 aye, jati sampurna   \n",
       "2      2               setu siung 119 rt 5 1 13880 cipayung   \n",
       "3      3                               toko dita, kertosono   \n",
       "4      4                                      jl. orde baru   \n",
       "..   ...                                                ...   \n",
       "995  995                             pawon solo, aryo beba,   \n",
       "996  996                  mawar iii, no 19 telukjambe timur   \n",
       "997  997            soek - hatta 60 kepuhkembeng peterongan   \n",
       "998  998  yaya pelayanan halieluyah, tebet raya, 30d rw ...   \n",
       "999  999              purman tanj beri no al 0 4 pasar rebo   \n",
       "\n",
       "                                    POI/street  \n",
       "0    /jl kapuk timur delta sili iii lippo cika  \n",
       "1                                            /  \n",
       "2                                       /siung  \n",
       "3                                   toko dita/  \n",
       "4                               /jl. orde baru  \n",
       "..                                         ...  \n",
       "995                       pawon solo/aryo beba  \n",
       "996                                 /mawar iii  \n",
       "997                              /soek - hatta  \n",
       "998    yayasan pelayanan halieluyah/tebet raya  \n",
       "999                                      /beri  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_bio_tags(text, poi_street):\n",
    "    poi, street = poi_street.split('/')\n",
    "    raw_tokens = text.split()\n",
    "    poi_tokens = poi.split()\n",
    "    street_tokens = street.split()\n",
    "\n",
    "    bio_tags = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(raw_tokens):\n",
    "        token = raw_tokens[i]\n",
    "        \n",
    "        if token in poi_tokens:\n",
    "            if i == 0 or bio_tags[-1] == 'O':\n",
    "                bio_tags.append('B-POI')\n",
    "            else:\n",
    "                bio_tags.append('I-POI')\n",
    "        elif token in street_tokens:\n",
    "            if i == 0 or bio_tags[-1] == 'O':\n",
    "                bio_tags.append('B-Street')\n",
    "            else:\n",
    "                bio_tags.append('I-Street')\n",
    "        else:\n",
    "            bio_tags.append('O')\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return raw_tokens, bio_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.apply(lambda row: create_bio_tags(row['raw_address'], row['POI/street'])[0], axis=1)\n",
    "df['bio_tags'] = df.apply(lambda row: create_bio_tags(row['raw_address'], row['POI/street'])[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.raw_address.values)\n",
    "tags = list(df.bio_tags.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n\u001b[0;32m     33\u001b[0m label_all_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[1;34m(sentences, labels)\u001b[0m\n\u001b[0;32m     22\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Ignore [CLS] and [SEP] tokens\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word_idx \u001b[38;5;241m!=\u001b[39m previous_word_idx:\n\u001b[1;32m---> 24\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(label_map[\u001b[43mlabel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(label_map[label[word_idx]] \u001b[38;5;28;01mif\u001b[39;00m label_all_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Define a label map\n",
    "label_list = ['B-POI', 'B-Street', 'I-POI', 'I-Street', 'O']\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Function to align labels with tokens\n",
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, is_split_into_words=True)\n",
    "    aligned_labels = []\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignore [CLS] and [SEP] tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_map[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "label_all_tokens = True\n",
    "tokenized_inputs = tokenize_and_align_labels(sentences, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-POI': 0, 'B-Street': 1, 'I-POI': 2, 'I-Street': 3, 'O': 4}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Combine the tokenized inputs and labels into a dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': [input['input_ids'] for input in tokenized_inputs],\n",
    "    'attention_mask': [input['attention_mask'] for input in tokenized_inputs],\n",
    "    'labels': tokenized_labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_list))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Make predictions on new sentences\n",
    "new_sentences = [\"Jl Karet Pedurenan No 39, depan masjid al mukminin\"]\n",
    "tokenized_inputs = tokenizer(new_sentences, truncation=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**tokenized_inputs)\n",
    "\n",
    "logits = output.logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "# Convert predictions to tags\n",
    "pred_tags = [[label_list[p] for p in pred] for pred in predictions]\n",
    "print(pred_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
